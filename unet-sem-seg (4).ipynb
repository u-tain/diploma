{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport random\nimport pickle\nimport torch\nfrom matplotlib import pyplot as plt \nimport torchvision.models as models\nfrom torchvision import transforms as T\nimport torchvision.transforms.functional as TF\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom tqdm import tqdm, tqdm_notebook\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom pathlib import Path\nfrom IPython.display import clear_output\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"527c5b86-199c-47c0-817d-fdaf62e989eb","_cell_guid":"aee6e758-eacb-4feb-99d0-ceb3a28d42b2","execution":{"iopub.status.busy":"2022-05-22T14:34:13.766341Z","iopub.execute_input":"2022-05-22T14:34:13.766895Z","iopub.status.idle":"2022-05-22T14:34:33.819084Z","shell.execute_reply.started":"2022-05-22T14:34:13.766819Z","shell.execute_reply":"2022-05-22T14:34:33.818343Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\nd = {'ImageId': df['ImageId'].unique()}\ntrain_df = pd.DataFrame(d)\ntrain_df['ClassesList'] = ['' for i in range(len(train_df))]\ntrain_df['EncodedPixels'] = ['' for i in range(len(train_df))]\nfor i in range(len(train_df)):\n    train_df['ClassesList'][i] = df.loc[df['ImageId']==train_df['ImageId'][i]]['ClassId'].values.tolist()\n    train_df['EncodedPixels'][i] = df.loc[df['ImageId']==train_df['ImageId'][i]]['EncodedPixels'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:33.821843Z","iopub.execute_input":"2022-05-22T14:34:33.822049Z","iopub.status.idle":"2022-05-22T14:34:57.180061Z","shell.execute_reply.started":"2022-05-22T14:34:33.822021Z","shell.execute_reply":"2022-05-22T14:34:57.179275Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def make_mask( encoded, shape=(1600,256)):\n    # Делим на два списка в соответствии с кодировкой\n    if isinstance(encoded, str):\n        encoded = list(map(int, encoded.split(' ')))\n    full,pixel,number = [],[],[]\n    [pixel.append(encoded[i]) if i%2==0 else number.append(encoded[i]) for i in range(0, len(encoded))]\n    # \"Раскрываем\" кодировку, получаем индексы закрашенных пикселей\n    k=0\n    for i in range(len(number)):\n        for j in range(number[i]):\n            ind = pixel[i]+j\n            full.append(ind-1)\n        k +=number[i]\n    # Создаем массив под готовое изображение    \n    mask = np.zeros((1600*256,1), dtype=int)\n    # Закрашиваем соответствующие пиксели\n    mask[full] = 255\n    #преобразем к размерам фотографий металла\n    res = np.reshape(mask,(1600, 256)).T\n    res = Image.fromarray(res.astype(np.uint8))\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.181812Z","iopub.execute_input":"2022-05-22T14:34:57.182100Z","iopub.status.idle":"2022-05-22T14:34:57.191044Z","shell.execute_reply.started":"2022-05-22T14:34:57.182059Z","shell.execute_reply":"2022-05-22T14:34:57.190067Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.192932Z","iopub.execute_input":"2022-05-22T14:34:57.193243Z","iopub.status.idle":"2022-05-22T14:34:57.262369Z","shell.execute_reply.started":"2022-05-22T14:34:57.193204Z","shell.execute_reply":"2022-05-22T14:34:57.261545Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# разные режимы датасета \nDATA_MODES = ['train', 'val', 'test']\n# все изображения будут масштабированы к размеру 224x224 px\nRESCALE_SIZE_1 = 800\nRESCALE_SIZE_2 = 128\n# работаем на видеокарте\nDEVICE = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.264296Z","iopub.execute_input":"2022-05-22T14:34:57.265928Z","iopub.status.idle":"2022-05-22T14:34:57.272249Z","shell.execute_reply.started":"2022-05-22T14:34:57.265889Z","shell.execute_reply":"2022-05-22T14:34:57.271475Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SteelDataset(Dataset):\n    def __init__(self, names, df, mode):\n        super().__init__()\n        self.names = names\n        self.mode = mode\n        if self.mode != 'test':\n            self.df = df\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.names)\n        \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file, mode):\n        if mode == 'test':\n            image = Image.open('../input/severstal-steel-defect-detection/test_images/'+ file).convert(\"RGB\")\n        else:\n            image = Image.open('../input/severstal-steel-defect-detection/train_images/'+ file).convert(\"RGB\")\n        image.load()\n        return image\n    \n    def __getitem__(self, index):\n            transforms_tens = T.Compose([\n            T.ToTensor()])\n\n            # загружаем и меняем размер изображения\n            img = self._prepare_sample(self.load_sample(self.names[index], self.mode))\n\n            if self.mode == 'test':\n                img = np.array(img)\n                max_value = 256 ** ((img.dtype == np.uint16) + 1) - 1\n                img = (img / max_value).astype(np.float32)\n                img = transforms_tens(img)\n                return img\n            \n            # загружаем классы и маски\n            labels = list(self.df['ClassesList'].loc[self.df['ImageId'] == self.names[index]])[0]\n            mask = list(self.df['EncodedPixels'].loc[self.df['ImageId'] == self.names[index]])[0]\n            num_obj = len(labels)\n            masks = np.zeros((RESCALE_SIZE_2, RESCALE_SIZE_1, 4), dtype=np.float32)\n            masks = np.transpose(masks,(2, 0, 1))\n            for i in range(4):\n                for j in range(num_obj):\n                    if i == (labels[j]-1):\n                # раскодирование масок\n                        masks[i] = np.array(self._prepare_sample(make_mask(mask[j])))\n                        masks[i] = np.array(masks[i])\n                        masks[i] = masks[i] / 255\n\n            # преобазование в тензоры\n            masks = torch.as_tensor(masks, dtype=torch.float32)\n            label = torch.as_tensor(labels)\n            \n            # преобразуем изображения и маски\n            if self.mode == \"train\":\n                img, masks= transforms_all(img,masks)\n                \n            img = np.array(img)\n            max_value = 256 ** ((img.dtype == np.uint16) + 1) - 1\n            img = (img / max_value).astype(np.float32)\n            img = transforms_tens(img)\n            \n            return img, masks\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE_1, RESCALE_SIZE_2))\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.273483Z","iopub.execute_input":"2022-05-22T14:34:57.273778Z","iopub.status.idle":"2022-05-22T14:34:57.295264Z","shell.execute_reply.started":"2022-05-22T14:34:57.273741Z","shell.execute_reply":"2022-05-22T14:34:57.294606Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def transforms_all(image, segmentation):\n    if random.random() > 0.5:\n        image = TF.autocontrast(image)\n    if random.random() > 0.5:\n        image = TF.hflip(image)\n        segmentation = TF.hflip(segmentation)\n    if random.random() > 0.5:\n        image = TF.vflip(image)\n        segmentation = TF.vflip(segmentation)\n    return image, segmentation","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:03.743665Z","iopub.execute_input":"2022-05-22T14:35:03.744423Z","iopub.status.idle":"2022-05-22T14:35:03.750759Z","shell.execute_reply.started":"2022-05-22T14:35:03.744375Z","shell.execute_reply":"2022-05-22T14:35:03.749654Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_val_names = train_df['ImageId']\ntrain_files,val_files = train_test_split(train_val_names, train_size=0.75)\nval_dataset = SteelDataset(list(val_files), train_df,  mode='val')\ntrain_dataset = SteelDataset(list(train_files),train_df, mode='train')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:06.313526Z","iopub.execute_input":"2022-05-22T14:35:06.314077Z","iopub.status.idle":"2022-05-22T14:35:06.366071Z","shell.execute_reply.started":"2022-05-22T14:35:06.314040Z","shell.execute_reply":"2022-05-22T14:35:06.365380Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# k = int(np.random.uniform(0,50))\n# res = train_dataset[k]\n\n# plt.show()\n# plt.figure(figsize=(13, 9))\n# plt.imshow(np.rollaxis(res[0].numpy(), 0, 3), cmap='gray')\n# plt.title('real')\n# plt.axis('off')\n# plt.show()\n\n# plt.figure(figsize=(13, 9))\n# plt.imshow(res[1].numpy())\n# plt.title('True')\n# plt.axis('off')\n# plt.show()\n# # print(res[2])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:24.479650Z","iopub.execute_input":"2022-05-22T14:35:24.479934Z","iopub.status.idle":"2022-05-22T14:35:24.858166Z","shell.execute_reply.started":"2022-05-22T14:35:24.479903Z","shell.execute_reply":"2022-05-22T14:35:24.856919Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset[k][1].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:54:49.078418Z","iopub.execute_input":"2022-05-18T19:54:49.078716Z","iopub.status.idle":"2022-05-18T19:54:49.14761Z","shell.execute_reply.started":"2022-05-18T19:54:49.078683Z","shell.execute_reply":"2022-05-18T19:54:49.146728Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n    \nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        self.double_conv = DoubleConv(in_channels, out_channels)\n        self.down_sample = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        skip_out = self.double_conv(x)\n        down_out = self.down_sample(skip_out)\n        return (down_out, skip_out)\n\n    \nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, up_sample_mode):\n        super(UpBlock, self).__init__()\n        if up_sample_mode == 'conv_transpose':\n            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n        elif up_sample_mode == 'bilinear':\n            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n        self.double_conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, down_input, skip_input):\n        x = self.up_sample(down_input)\n        x = torch.cat([x, skip_input], dim=1)\n        return self.double_conv(x)\n\n    \nclass UNet(nn.Module):\n    def __init__(self, out_classes=4, up_sample_mode='conv_transpose'):\n        super(UNet, self).__init__()\n        self.up_sample_mode = up_sample_mode\n        # Downsampling Path\n        self.down_conv1 = DownBlock(3, 64)\n        self.down_conv2 = DownBlock(64, 128)\n        self.down_conv3 = DownBlock(128, 256)\n        self.down_conv4 = DownBlock(256, 512)\n        # Bottleneck\n        self.double_conv = DoubleConv(512, 1024)\n        # Upsampling Path\n        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n        # Final Convolution\n        self.conv_last = nn.Conv2d(64, 4, kernel_size=1)\n\n    def forward(self, x):\n        x, skip1_out = self.down_conv1(x)\n        x, skip2_out = self.down_conv2(x)\n        x, skip3_out = self.down_conv3(x)\n        x, skip4_out = self.down_conv4(x)\n        x = self.double_conv(x)\n        x = self.up_conv4(x, skip4_out)\n        x = self.up_conv3(x, skip3_out)\n        x = self.up_conv2(x, skip2_out)\n        x = self.up_conv1(x, skip1_out)\n        x = self.conv_last(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:30.805972Z","iopub.execute_input":"2022-05-22T14:35:30.806221Z","iopub.status.idle":"2022-05-22T14:35:30.823804Z","shell.execute_reply.started":"2022-05-22T14:35:30.806192Z","shell.execute_reply":"2022-05-22T14:35:30.823067Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, batch_size):\n    model.train()\n    running_loss = 0.0\n    dice = 0.0\n    iou = 0.0\n    for X_batch, Y_batch in train_loader:\n        inputs = X_batch.to(DEVICE)\n        labels = Y_batch.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs,labels.long())\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.detach().cpu().numpy() \n        dice += dice_metric(outputs, labels)\n        iou += mIoU(outputs, labels)\n    train_loss = running_loss / len(train_loader)\n    train_dice = dice / len(train_loader)\n    train_IoU = iou / len(train_loader)\n    return train_loss, train_dice, train_IoU","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:52:43.740592Z","iopub.execute_input":"2022-05-18T19:52:43.741161Z","iopub.status.idle":"2022-05-18T19:52:43.748768Z","shell.execute_reply.started":"2022-05-18T19:52:43.741121Z","shell.execute_reply":"2022-05-18T19:52:43.747729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_epoch(model, val_loader, criterion, batch_size):\n    model.eval()\n    dice = 0.0\n    iou = 0.0\n    running_loss = 0.0\n    for X_batch, Y_batch in val_loader:\n        inputs = X_batch.to(DEVICE)\n        labels = Y_batch.to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs,labels.long())\n            running_loss += loss.item()\n            dice += dice_metric(outputs, labels)\n            iou += mIoU(outputs, labels)\n    val_loss = running_loss / len(val_loader)\n    val_dice = dice / len(val_loader)\n    val_IoU = iou / len(val_loader)\n    return val_loss, val_dice, val_IoU","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:52:39.614759Z","iopub.execute_input":"2022-05-18T19:52:39.615568Z","iopub.status.idle":"2022-05-18T19:52:39.62513Z","shell.execute_reply.started":"2022-05-18T19:52:39.615528Z","shell.execute_reply":"2022-05-18T19:52:39.623476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size,num_workers = 2, shuffle=True)#,collate_fn = collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers = 2, shuffle=False)#,collate_fn = collate_fn)\n    history = []\n    maxIoU = 0\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} train_dice: {t_dice:0.4f} train_IoU: {t_iou:0.4f}\\\n    \\nValidation  val_loss: {v_loss:0.4f} val_dice: {v_dice:0.4f} val_IoU: {v_iou: 0.4f}\"\n    criterion = nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss()\n    \n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        params = model.parameters()\n        opt = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(opt,\n                                                   step_size=20,\n                                                   gamma=0.1)\n        for epoch in range(epochs):\n            train_loss, train_dice, train_IoU = fit_epoch(model, train_loader, criterion, opt, batch_size)\n            lr_scheduler.step()    \n            val_loss, val_dice, val_IoU = eval_epoch(model, val_loader, criterion, batch_size)\n            history.append((train_loss, train_dice, train_IoU, val_loss, val_dice, val_IoU))\n            if val_IoU > maxIoU:\n                maxIoU = val_IoU\n                torch.save(model, './weight_unet.dat')\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, t_dice = train_dice, t_iou = train_IoU, \\\n                                           v_loss=val_loss, v_dice = val_acc, v_iou = val_IoU ))   \n    return history","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-18T19:40:32.38369Z","iopub.execute_input":"2022-05-18T19:40:32.384044Z","iopub.status.idle":"2022-05-18T19:40:32.399085Z","shell.execute_reply.started":"2022-05-18T19:40:32.384007Z","shell.execute_reply":"2022-05-18T19:40:32.398032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_ious(pred, label, classes=[1], ignore_index=255, only_present=True):\n    '''computes iou for one ground truth mask and predicted mask'''\n    pred[label == ignore_index] = 0\n    ious = []\n    for c in classes:\n        label_c = label == c\n        if only_present and np.sum(label_c) == 0:\n            ious.append(np.nan)\n            continue\n        pred_c = pred == c\n        intersection = np.logical_and(pred_c, label_c).sum()\n        union = np.logical_or(pred_c, label_c).sum()\n        if union != 0:\n            ious.append(intersection / union)\n    return ious if ious else [1]\n\ndef mIoU(outputs, labels, classes=None):\n    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n    ious = []\n    preds = np.copy(outputs) # copy is imp\n    labels = np.array(labels) # tensor to np\n    for pred, label in zip(preds, labels):\n        ious.append(np.nanmean(compute_ious(pred, label, classes=[1])))\n    iou = np.nanmean(ious)\n    return iou\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:39:37.196131Z","iopub.execute_input":"2022-05-18T19:39:37.19663Z","iopub.status.idle":"2022-05-18T19:39:37.209968Z","shell.execute_reply.started":"2022-05-18T19:39:37.196586Z","shell.execute_reply":"2022-05-18T19:39:37.208226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n#     with torch.no_grad():\n#         pred_mask = F.softmax(pred_mask, dim=1)\n#         pred_mask = torch.argmax(pred_mask, dim=1)\n#         pred_mask = pred_mask.contiguous().view(-1)\n#         mask = mask.contiguous().view(-1)\n\n#         iou_per_class = []\n#         for clas in range(0, n_classes): #loop per pixel class\n#             true_class = pred_mask == clas\n#             true_label = mask == clas\n\n#             if true_label.long().sum().item() == 0: #no exist label in this loop\n#                 iou_per_class.append(np.nan)\n#             else:\n#                 intersect = torch.logical_and(true_class, true_label).sum().float().item()\n#                 union = torch.logical_or(true_class, true_label).sum().float().item()\n\n#                 iou = (intersect + smooth) / (union +smooth)\n#                 iou_per_class.append(iou)\n#         return np.nanmean(iou_per_class)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:38.053571Z","iopub.execute_input":"2022-05-22T14:35:38.054301Z","iopub.status.idle":"2022-05-22T14:35:38.061894Z","shell.execute_reply.started":"2022-05-22T14:35:38.054259Z","shell.execute_reply":"2022-05-22T14:35:38.061158Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# def dice_metric1(y_pred,y_true, smooth=1):\n# #     print(y_pred.shape)\n#     y_pred = F.softmax(y_pred, dim=1)\n#     y_pred = torch.argmax(y_pred, dim=1)\n# #     y_pred_f = y_pred.contiguous().view( -1)\n# #     y_true_f = y_true.contiguous().view(-1)\n#     y_true_f = torch.flatten(y_true)\n#     y_pred_f = torch.flatten(y_pred)\n#     intersection = torch.sum(y_true_f.float() * y_pred_f.float())\n#     return (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f.float()) + smooth)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:46.139960Z","iopub.execute_input":"2022-05-22T14:35:46.140222Z","iopub.status.idle":"2022-05-22T14:35:46.146077Z","shell.execute_reply.started":"2022-05-22T14:35:46.140194Z","shell.execute_reply":"2022-05-22T14:35:46.145340Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def dice_metric(probability, truth, threshold=0.5, reduction='none'):\n    batch_size = len(truth)\n    with torch.no_grad():\n        probability = probability.view(batch_size, -1)\n        truth = truth.view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = (probability > threshold).float()\n        t = truth.float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        neg_index = torch.nonzero(t_sum == 0)\n        pos_index = torch.nonzero(t_sum >= 1)\n\n        dice_neg = (p_sum == 0).float()\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_neg = dice_neg[neg_index]\n        dice_pos = dice_pos[pos_index]\n        dice = dice_pos\n    return dice","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:39:40.466779Z","iopub.execute_input":"2022-05-18T19:39:40.467336Z","iopub.status.idle":"2022-05-18T19:39:40.475114Z","shell.execute_reply.started":"2022-05-18T19:39:40.467281Z","shell.execute_reply":"2022-05-18T19:39:40.473962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T20:08:13.170948Z","iopub.execute_input":"2022-05-18T20:08:13.171527Z","iopub.status.idle":"2022-05-18T20:08:13.525797Z","shell.execute_reply.started":"2022-05-18T20:08:13.17149Z","shell.execute_reply":"2022-05-18T20:08:13.524483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class conv_block_nested(nn.Module):\n    \n#     def __init__(self, in_ch, mid_ch, out_ch):\n#         super(conv_block_nested, self).__init__()\n#         self.activation = nn.ReLU(inplace=True)\n#         self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n#         self.bn1 = nn.BatchNorm2d(mid_ch)\n#         self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n#         self.bn2 = nn.BatchNorm2d(out_ch)\n\n#     def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.bn1(x)\n#         x = self.activation(x)\n        \n#         x = self.conv2(x)\n#         x = self.bn2(x)\n#         output = self.activation(x)\n\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.92862Z","iopub.status.idle":"2022-05-18T00:08:33.929012Z","shell.execute_reply.started":"2022-05-18T00:08:33.928795Z","shell.execute_reply":"2022-05-18T00:08:33.928819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class NestedUNet(nn.Module):\n\n#     def __init__(self, in_ch=3, out_ch=5):\n#         super(NestedUNet, self).__init__()\n\n#         n1 = 64\n#         filters = [64,128, 256, 512, 1024]\n\n#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n#         self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n#         self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n#         self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n#         self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n#         self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n#         self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n\n#         self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n#         self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n#         self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n#         self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n\n#         self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n#         self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n#         self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n\n#         self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n#         self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n\n#         self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n\n#         self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n\n\n#     def forward(self, x):\n        \n#         x0_0 = self.conv0_0(x)\n#         x1_0 = self.conv1_0(self.pool(x0_0))\n#         x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n\n#         x2_0 = self.conv2_0(self.pool(x1_0))\n#         x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n#         x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n\n#         x3_0 = self.conv3_0(self.pool(x2_0))\n#         x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n#         x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n#         x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n\n#         x4_0 = self.conv4_0(self.pool(x3_0))\n#         x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n#         x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n#         x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n#         x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n\n#         output = self.final(x0_4)\n#         return output\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.931198Z","iopub.status.idle":"2022-05-18T00:08:33.931828Z","shell.execute_reply.started":"2022-05-18T00:08:33.9316Z","shell.execute_reply":"2022-05-18T00:08:33.931624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = NestedUNet().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.932864Z","iopub.status.idle":"2022-05-18T00:08:33.934674Z","shell.execute_reply.started":"2022-05-18T00:08:33.934427Z","shell.execute_reply":"2022-05-18T00:08:33.934453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class UnetConv2dBlock(nn.Module):\n\n#     def __init__(self, in_channel: int, mid_channel: int, out_channel: int, kernel_size: list = [3,3], stride_size: list = [1,1], activation: str = 'relu'):\n#         super().__init__()\n\n#         if activation == 'swish':\n#             self.conv2dblock = nn.Sequential(\n#             nn.Conv2d(in_channel, mid_channel, kernel_size=kernel_size[0], stride=stride_size[0], padding=1),\n#             nn.BatchNorm2d(mid_channel),\n#             Swish(),\n#             nn.Conv2d(mid_channel, out_channel, kernel_size=kernel_size[1], stride=stride_size[1], padding=1),\n#             nn.BatchNorm2d(out_channel),\n#             Swish(),\n#         )\n#         else:\n#             self.conv2dblock = nn.Sequential(\n#                 nn.Conv2d(in_channel, mid_channel, kernel_size=kernel_size[0], stride=stride_size[0], padding=1),\n#                 nn.BatchNorm2d(mid_channel),\n#                 nn.ReLU(inplace=True),\n#                 nn.Conv2d(mid_channel, out_channel, kernel_size=kernel_size[1], stride=stride_size[1], padding=1),\n#                 nn.BatchNorm2d(out_channel),\n#                 nn.ReLU(inplace=True)\n#             )\n\n#     def forward(self, x):\n#         return self.conv2dblock(x)\n\n\n\n# class Swish(nn.Module):\n\n#     def __init__(self, slope = 1):\n#         super().__init__()\n#         self.slope = slope # * torch.nn.Parameter(torch.ones(1))\n\n#     def forward(self, x):\n#         return self.slope * x * torch.sigmoid(x)\n\n\n\n# class DownSample(nn.Module):\n\n#     def __init__(self):\n#         super().__init__()\n#         self.maxpooling = nn.MaxPool2d(2)\n\n#     def forward(self, x):\n#         return self.maxpooling(x)\n\n\n\n# class UpSample(nn.Module):\n\n#     def __init__(self, in_channel=None, bilinear: bool = True):\n#         super().__init__()\n\n#         if bilinear:\n#             self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n#         else:\n#             self.up = nn.ConvTranspose2d(in_channel , in_channel // 2, kernel_size=2, stride=2)\n\n\n#     def forward(self, x1, x2):\n#         x1 = self.up(x1)\n        \n#         diffY = x2.size()[2] - x1.size()[2]\n#         diffX = x2.size()[3] - x1.size()[3]\n\n#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n#                         diffY // 2, diffY - diffY // 2])\n#         x = torch.cat([x2, x1], dim=1)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.936058Z","iopub.status.idle":"2022-05-18T00:08:33.93647Z","shell.execute_reply.started":"2022-05-18T00:08:33.936247Z","shell.execute_reply":"2022-05-18T00:08:33.93627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class UnetPlusPlus(nn.Module):\n    \n#     def __init__(self, in_channel, num_classes: int, deep_supervision: bool=False):\n#         super(UnetPlusPlus, self).__init__()\n#         self.num_classes = num_classes\n#         self.metrics = 0 \n#         self.deep_supervision = deep_supervision\n\n#         num_filter = [32, 64, 128, 256, 512]\n\n#         self.down = DownSample()\n#         self.up = UpSample()\n\n#         self.conv_block_0_0 = UnetConv2dBlock(in_channel, num_filter[0], num_filter[0])\n#         self.conv_block_1_0 = UnetConv2dBlock(num_filter[0], num_filter[1], num_filter[1])\n#         self.conv_block_2_0 = UnetConv2dBlock(num_filter[1], num_filter[2], num_filter[2])\n#         self.conv_block_3_0 = UnetConv2dBlock(num_filter[2], num_filter[3], num_filter[3])\n        \n#         self.conv_block_0_1 = UnetConv2dBlock(num_filter[0]+num_filter[1], num_filter[0], num_filter[0])\n#         self.conv_block_1_1 = UnetConv2dBlock(num_filter[1]+num_filter[2], num_filter[1], num_filter[1])\n#         self.conv_block_2_1 = UnetConv2dBlock(num_filter[2]+num_filter[3], num_filter[2], num_filter[2])\n\n#         self.conv_block_0_2 = UnetConv2dBlock(2*num_filter[0]+num_filter[1], num_filter[0], num_filter[0])\n#         self.conv_block_1_2 = UnetConv2dBlock(2*num_filter[1]+num_filter[2], num_filter[1], num_filter[1])\n\n#         self.conv_block_0_3 = UnetConv2dBlock(3*num_filter[0]+num_filter[1], num_filter[0], num_filter[0])\n\n#         if self.deep_supervision:\n#             self.final1 = nn.Conv2d(num_filter[0], self.num_classes, kernel_size=1)\n#             self.final2 = nn.Conv2d(num_filter[0], self.num_classes, kernel_size=1)\n#             self.final3 = nn.Conv2d(num_filter[0], self.num_classes, kernel_size=1)\n#         else:\n#             self.final = nn.Conv2d(num_filter[0], self.num_classes, kernel_size=1)\n\n#     def forward(self, x):\n\n#         x0_0 = self.conv_block_0_0(x)\n\n#         x1_0 = self.conv_block_1_0(self.down(x0_0))\n#         x0_1 = self.conv_block_0_1(self.up(x1_0, x0_0))\n        \n#         x2_0 = self.conv_block_2_0(self.down(x1_0))\n#         x1_1 = self.conv_block_1_1(self.up(x2_0, x1_0))\n#         x0_2 = self.conv_block_0_2(self.up(x1_1, torch.cat([x0_0, x0_1], 1)))\n\n#         x3_0 = self.conv_block_3_0(self.down(x2_0))\n#         x2_1 = self.conv_block_2_1(self.up(x3_0, x2_0))\n#         x1_2 = self.conv_block_1_2(self.up(x2_1, torch.cat([x1_0, x1_1],1)))\n#         x0_3 = self.conv_block_0_3(self.up(x1_2, torch.cat([x0_0, x0_1, x0_2], 1)))\n\n#         if self.deep_supervision:\n#             output1 = self.final1(x0_1)\n#             output2 = self.final2(x0_2)\n#             output3 = self.final3(x0_3)\n#             return [output1, output2, output3]\n\n#         else:\n#             out = self.final(x0_3)\n#             return out","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.937883Z","iopub.status.idle":"2022-05-18T00:08:33.938285Z","shell.execute_reply.started":"2022-05-18T00:08:33.938057Z","shell.execute_reply":"2022-05-18T00:08:33.938078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = UnetPlusPlus(3,5).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.939604Z","iopub.status.idle":"2022-05-18T00:08:33.939991Z","shell.execute_reply.started":"2022-05-18T00:08:33.939775Z","shell.execute_reply":"2022-05-18T00:08:33.939798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = train(train_dataset, val_dataset, model, epochs = 36, batch_size = 5)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T20:08:28.796432Z","iopub.execute_input":"2022-05-18T20:08:28.797644Z","iopub.status.idle":"2022-05-18T20:08:29.800086Z","shell.execute_reply.started":"2022-05-18T20:08:28.797594Z","shell.execute_reply":"2022-05-18T20:08:29.798679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history1 = train(train_dataset, val_dataset, model, epochs = 10, batch_size = 10)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.94327Z","iopub.status.idle":"2022-05-18T00:08:33.943663Z","shell.execute_reply.started":"2022-05-18T00:08:33.94345Z","shell.execute_reply":"2022-05-18T00:08:33.943473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_acc, train_IoU, val_loss, val_acc, val_IoU = zip(*history)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.945042Z","iopub.status.idle":"2022-05-18T00:08:33.945444Z","shell.execute_reply.started":"2022-05-18T00:08:33.945216Z","shell.execute_reply":"2022-05-18T00:08:33.945251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loss1, train_acc1, train_IoU1, val_loss1, val_acc1, val_IoU1 = zip(*history1)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.946804Z","iopub.status.idle":"2022-05-18T00:08:33.947191Z","shell.execute_reply.started":"2022-05-18T00:08:33.946979Z","shell.execute_reply":"2022-05-18T00:08:33.947001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ttrain_loss = []\n# ttrain_acc = []\n# ttrain_IoU =[]\n# tval_loss =[]\n# tval_acc =[]\n# tval_IoU=[]\n# for i in range(40):\n#     ttrain_loss.append(train_loss[i])\n#     ttrain_acc.append(train_acc[i]) \n#     ttrain_IoU.append(train_IoU[i]) \n#     tval_loss.append(val_loss[i]) \n#     tval_acc.append(val_acc[i]) \n#     tval_IoU.append(val_IoU[i])\n# for i in range(4):\n#     ttrain_loss.append(train_loss1[i])\n#     ttrain_acc.append(train_acc1[i]) \n#     ttrain_IoU.append(train_IoU1[i]) \n#     tval_loss.append(val_loss1[i]) \n#     tval_acc.append(val_acc1[i]) \n#     tval_IoU.append(val_IoU1[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.948613Z","iopub.status.idle":"2022-05-18T00:08:33.949006Z","shell.execute_reply.started":"2022-05-18T00:08:33.948791Z","shell.execute_reply":"2022-05-18T00:08:33.948814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(30,13))\n#     ax1.plot(train_loss, label='train', marker='o')\n#     ax1.plot(val_loss, label='val', marker='o')\n#     ax1.set_title('Loss per epoch')\n#     ax1.set_ylabel('loss');\n#     ax1.set_xlabel('epoch')\n#     ax1.legend(), ax1.grid()\n    \n#     ax2.plot(train_IoU, label='train_IoU', marker='*')\n#     ax2.plot(val_IoU, label='val_IoU',  marker='*')\n#     ax2.set_title('Score per epoch')\n#     ax2.set_ylabel('mean IoU')\n#     ax2.set_xlabel('epoch')\n#     ax2.legend(), ax2.grid()\n\n#     ax3.plot(train_acc, label='train_accuracy', marker='*')\n#     ax3.plot(val_acc, label='val_accuracy',  marker='*')\n#     ax3.set_title('Accuracy per epoch')\n#     ax3.set_ylabel('Accuracy')\n#     ax3.set_xlabel('epoch')\n#     ax3.legend(), ax3.grid()\n# #     plt.savefig('0.710615.png')\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.950456Z","iopub.status.idle":"2022-05-18T00:08:33.950846Z","shell.execute_reply.started":"2022-05-18T00:08:33.950634Z","shell.execute_reply":"2022-05-18T00:08:33.950656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image_mask_score(model, image, mask, metric = dice_metric):\n    model.eval()\n    \n    image = image.to(DEVICE)\n    mask = mask.to(DEVICE)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n#         print(output)\n        score = metric(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0)\n    return masked, score","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:04:53.049299Z","iopub.execute_input":"2022-05-22T15:04:53.049848Z","iopub.status.idle":"2022-05-22T15:04:53.055660Z","shell.execute_reply.started":"2022-05-22T15:04:53.049807Z","shell.execute_reply":"2022-05-22T15:04:53.054928Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def score(model, dataset, metric = dice_metric):\n    res_score = []\n    for i in tqdm(range(len(val_dataset))):\n        img, mask = dataset[i]\n        pred_mask, score = predict_image_mask_score(model, img, mask, metric)\n        res_score.append(score)\n    return res_score","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:07:07.750157Z","iopub.execute_input":"2022-05-22T15:07:07.750498Z","iopub.status.idle":"2022-05-22T15:07:07.758656Z","shell.execute_reply.started":"2022-05-22T15:07:07.750457Z","shell.execute_reply":"2022-05-22T15:07:07.757568Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def dice_metric(prob, true, threshold=0.5, reduction='none'):\n    '''Calculates dice of positive and negative images seperately'''\n    '''probability and truth must be torch tensors'''\n    batch_size = len(true)\n    prob = F.softmax(prob, dim=1)\n    prob = torch.argmax(prob, dim=1)\n#     print(prob)\n#     print(prob.shape)\n    probability = np.zeros((RESCALE_SIZE_2, RESCALE_SIZE_1, 4), dtype=np.float32)\n    probability = np.transpose(probability,(2, 0, 1))\n    truth = np.zeros((RESCALE_SIZE_2, RESCALE_SIZE_1, 4), dtype=np.float32)\n    truth = np.transpose(truth,(2, 0, 1))\n    for i in range(4):\n        for j in range(RESCALE_SIZE_2):\n            for k in range(RESCALE_SIZE_1):\n#                 print(prob[j][k])\n                if i == prob[0][j][k]-1:\n                    probability[i][j][k] = 1\n                if i == true[0][j][k]-1:\n                    truth[i][j][k] = 1\n    probability = torch.as_tensor(probability) \n    truth = torch.as_tensor(truth)\n    with torch.no_grad():\n        probability = probability.contiguous().view(batch_size, -1)\n        truth = truth.contiguous().view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = probability\n        t = truth \n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        neg_index = torch.nonzero(t_sum == 0)\n        pos_index = torch.nonzero(t_sum >= 1)\n\n        dice_neg = (p_sum == 0).float()\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_neg = dice_neg[neg_index]\n        dice_pos = dice_pos[pos_index]\n        dice = dice_pos\n\n    return dice","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:04:48.554088Z","iopub.execute_input":"2022-05-22T15:04:48.554357Z","iopub.status.idle":"2022-05-22T15:04:48.568028Z","shell.execute_reply.started":"2022-05-22T15:04:48.554326Z","shell.execute_reply":"2022-05-22T15:04:48.567190Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"model = torch.load('../input/unet-weight/weight_unet_wew.dat')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:36:14.672272Z","iopub.execute_input":"2022-05-22T14:36:14.672534Z","iopub.status.idle":"2022-05-22T14:36:18.892261Z","shell.execute_reply.started":"2022-05-22T14:36:14.672506Z","shell.execute_reply":"2022-05-22T14:36:18.891515Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(np.mean(score(model, val_dataset)))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:07:11.322781Z","iopub.execute_input":"2022-05-22T15:07:11.323139Z","iopub.status.idle":"2022-05-22T15:07:56.740215Z","shell.execute_reply.started":"2022-05-22T15:07:11.323091Z","shell.execute_reply":"2022-05-22T15:07:56.739079Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"k1,k2 = int(np.random.uniform(0,100)),int(np.random.uniform(0,100))\nimage1, mask1 = val_dataset[k1]\npred_mask1, score1 = predict_image_mask_score(model, image1, mask1)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(25,13))\nax1.imshow(np.rollaxis(image1.numpy(), 0, 3))\nax1.set_title('Picture');\nax1.set_axis_off()\n\nax2.imshow(mask1)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask1)\nax3.set_title('UNet | '+ str(score1))\nax3.set_axis_off()\nprint(score1)\nimage2, mask2 = val_dataset[k2]\npred_mask2, score2 = predict_image_mask_score(model, image2, mask2)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(25,13))\nax1.imshow(np.rollaxis(image2.numpy(), 0, 3))\nax1.set_title('Picture');\nax1.set_axis_off()\n\nax2.imshow(mask2)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask2)\nax3.set_title('UNet | '+str(score2))\nax3.set_axis_off()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:04:57.816333Z","iopub.execute_input":"2022-05-22T15:04:57.816617Z","iopub.status.idle":"2022-05-22T15:06:06.665585Z","shell.execute_reply.started":"2022-05-22T15:04:57.816570Z","shell.execute_reply":"2022-05-22T15:06:06.664863Z"},"trusted":true},"execution_count":71,"outputs":[]}]}