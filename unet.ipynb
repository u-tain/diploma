{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells": [{"cell_type":"code",   "source": "%matplotlib inline\nimport os\nimport torch\nimport random\nimport pickle\nimport numpy as np \nimport pandas as pd \nfrom PIL import Image\nimport torch.nn as nn \nfrom pathlib import Path\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt \nfrom tqdm import tqdm, tqdm_notebook\nfrom torchvision import transforms as T\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import Dataset, DataLoader\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"527c5b86-199c-47c0-817d-fdaf62e989eb","_cell_guid":"aee6e758-eacb-4feb-99d0-ceb3a28d42b2","execution":{"iopub.status.busy":"2022-05-22T14:34:13.766341Z","iopub.execute_input":"2022-05-22T14:34:13.766895Z","iopub.status.idle":"2022-05-22T14:34:33.819084Z","shell.execute_reply.started":"2022-05-22T14:34:13.766819Z","shell.execute_reply":"2022-05-22T14:34:33.818343Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\nd = {'ImageId': df['ImageId'].unique()}\ntrain_df = pd.DataFrame(d)\ntrain_df['ClassesList'] = ['' for i in range(len(train_df))]\ntrain_df['EncodedPixels'] = ['' for i in range(len(train_df))]\nfor i in range(len(train_df)):\n    train_df['ClassesList'][i] = df.loc[df['ImageId']==train_df['ImageId'][i]]['ClassId'].values.tolist()\n    train_df['EncodedPixels'][i] = df.loc[df['ImageId']==train_df['ImageId'][i]]['EncodedPixels'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:33.821843Z","iopub.execute_input":"2022-05-22T14:34:33.822049Z","iopub.status.idle":"2022-05-22T14:34:57.180061Z","shell.execute_reply.started":"2022-05-22T14:34:33.822021Z","shell.execute_reply":"2022-05-22T14:34:57.179275Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def make_mask( encoded, shape=(1600,256)):\n    # Делим на два списка в соответствии с кодировкой\n    if isinstance(encoded, str):\n        encoded = list(map(int, encoded.split(' ')))\n    full,pixel,number = [],[],[]\n    [pixel.append(encoded[i]) if i%2==0 else number.append(encoded[i]) for i in range(0, len(encoded))]\n    # \"Раскрываем\" кодировку, получаем индексы закрашенных пикселей\n    k=0\n    for i in range(len(number)):\n        for j in range(number[i]):\n            ind = pixel[i]+j\n            full.append(ind-1)\n        k +=number[i]\n    # Создаем массив под готовое изображение    \n    mask = np.zeros((1600*256,1), dtype=int)\n    # Закрашиваем соответствующие пиксели\n    mask[full] = 255\n    #преобразем к размерам фотографий металла\n    res = np.reshape(mask,(1600, 256)).T\n    res = Image.fromarray(res.astype(np.uint8))\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.181812Z","iopub.execute_input":"2022-05-22T14:34:57.182100Z","iopub.status.idle":"2022-05-22T14:34:57.191044Z","shell.execute_reply.started":"2022-05-22T14:34:57.182059Z","shell.execute_reply":"2022-05-22T14:34:57.190067Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.192932Z","iopub.execute_input":"2022-05-22T14:34:57.193243Z","iopub.status.idle":"2022-05-22T14:34:57.262369Z","shell.execute_reply.started":"2022-05-22T14:34:57.193204Z","shell.execute_reply":"2022-05-22T14:34:57.261545Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# разные режимы датасета \nDATA_MODES = ['train', 'val', 'test']\n# все изображения будут масштабированы к размеру 224x224 px\nRESCALE_SIZE_1 = 800\nRESCALE_SIZE_2 = 128\n# работаем на видеокарте\nDEVICE = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.264296Z","iopub.execute_input":"2022-05-22T14:34:57.265928Z","iopub.status.idle":"2022-05-22T14:34:57.272249Z","shell.execute_reply.started":"2022-05-22T14:34:57.265889Z","shell.execute_reply":"2022-05-22T14:34:57.271475Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SteelDataset(Dataset):\n    def __init__(self, names, df, mode):\n        super().__init__()\n        self.names = names\n        self.mode = mode\n        if self.mode != 'test':\n            self.df = df\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.names)\n        \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file, mode):\n        if mode == 'test':\n            image = Image.open('../input/severstal-steel-defect-detection/test_images/'+ file).convert(\"RGB\")\n        else:\n            image = Image.open('../input/severstal-steel-defect-detection/train_images/'+ file).convert(\"RGB\")\n        image.load()\n        return image\n    \n    def __getitem__(self, index):\n            transforms_tens = T.Compose([\n            T.ToTensor()])\n\n            # загружаем и меняем размер изображения\n            img = self._prepare_sample(self.load_sample(self.names[index], self.mode))\n\n            if self.mode == 'test':\n                img = np.array(img)\n                max_value = 256 ** ((img.dtype == np.uint16) + 1) - 1\n                img = (img / max_value).astype(np.float32)\n                img = transforms_tens(img)\n                return img\n            \n            # загружаем классы и маски\n            labels = list(self.df['ClassesList'].loc[self.df['ImageId'] == self.names[index]])[0]\n            mask = list(self.df['EncodedPixels'].loc[self.df['ImageId'] == self.names[index]])[0]\n            num_obj = len(labels)\n            masks = np.zeros((RESCALE_SIZE_2, RESCALE_SIZE_1, 4), dtype=np.float32)\n            masks = np.transpose(masks,(2, 0, 1))\n            for i in range(4):\n                for j in range(num_obj):\n                    if i == (labels[j]-1):\n                # раскодирование масок\n                        masks[i] = np.array(self._prepare_sample(make_mask(mask[j])))\n                        masks[i] = masks[i] / 255\n\n            # преобазование в тензоры\n            masks = torch.as_tensor(masks, dtype=torch.float32)\n            label = torch.as_tensor(labels)\n            \n            # преобразуем изображения и маски\n            if self.mode == \"train\":\n                img, masks= transforms_all(img,masks)\n                \n            img = np.array(img)\n            max_value = 256 ** ((img.dtype == np.uint16) + 1) - 1\n            img = (img / max_value).astype(np.float32)\n            img = transforms_tens(img)\n            \n            return img, masks\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE_1, RESCALE_SIZE_2))\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:34:57.273483Z","iopub.execute_input":"2022-05-22T14:34:57.273778Z","iopub.status.idle":"2022-05-22T14:34:57.295264Z","shell.execute_reply.started":"2022-05-22T14:34:57.273741Z","shell.execute_reply":"2022-05-22T14:34:57.294606Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def transforms_all(image, segmentation):\n    if random.random() > 0.5:\n        image = TF.autocontrast(image)\n    if random.random() > 0.5:\n        image = TF.hflip(image)\n        segmentation = TF.hflip(segmentation)\n    if random.random() > 0.5:\n        image = TF.vflip(image)\n        segmentation = TF.vflip(segmentation)\n    return image, segmentation","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:03.743665Z","iopub.execute_input":"2022-05-22T14:35:03.744423Z","iopub.status.idle":"2022-05-22T14:35:03.750759Z","shell.execute_reply.started":"2022-05-22T14:35:03.744375Z","shell.execute_reply":"2022-05-22T14:35:03.749654Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_val_names = train_df['ImageId']\ntrain_files,val_files = train_test_split(train_val_names, train_size=0.7)\nval_dataset = SteelDataset(list(val_files), train_df,  mode='val')\ntrain_dataset = SteelDataset(list(train_files),train_df, mode='train')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:06.313526Z","iopub.execute_input":"2022-05-22T14:35:06.314077Z","iopub.status.idle":"2022-05-22T14:35:06.366071Z","shell.execute_reply.started":"2022-05-22T14:35:06.314040Z","shell.execute_reply":"2022-05-22T14:35:06.365380Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n    \nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        self.double_conv = DoubleConv(in_channels, out_channels)\n        self.down_sample = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        skip_out = self.double_conv(x)\n        down_out = self.down_sample(skip_out)\n        return (down_out, skip_out)\n\n    \nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, up_sample_mode):\n        super(UpBlock, self).__init__()\n        if up_sample_mode == 'conv_transpose':\n            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n        elif up_sample_mode == 'bilinear':\n            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n        self.double_conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, down_input, skip_input):\n        x = self.up_sample(down_input)\n        x = torch.cat([x, skip_input], dim=1)\n        return self.double_conv(x)\n\n    \nclass UNet(nn.Module):\n    def __init__(self, out_classes=4, up_sample_mode='conv_transpose'):\n        super(UNet, self).__init__()\n        self.up_sample_mode = up_sample_mode\n        # Downsampling Path\n        self.down_conv1 = DownBlock(3, 64)\n        self.down_conv2 = DownBlock(64, 128)\n        self.down_conv3 = DownBlock(128, 256)\n        self.down_conv4 = DownBlock(256, 512)\n        # Bottleneck\n        self.double_conv = DoubleConv(512, 1024)\n        # Upsampling Path\n        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n        # Final Convolution\n        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n\n    def forward(self, x):\n        x, skip1_out = self.down_conv1(x)\n        x, skip2_out = self.down_conv2(x)\n        x, skip3_out = self.down_conv3(x)\n        x, skip4_out = self.down_conv4(x)\n        x = self.double_conv(x)\n        x = self.up_conv4(x, skip4_out)\n        x = self.up_conv3(x, skip3_out)\n        x = self.up_conv2(x, skip2_out)\n        x = self.up_conv1(x, skip1_out)\n        x = self.conv_last(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:35:30.805972Z","iopub.execute_input":"2022-05-22T14:35:30.806221Z","iopub.status.idle":"2022-05-22T14:35:30.823804Z","shell.execute_reply.started":"2022-05-22T14:35:30.806192Z","shell.execute_reply":"2022-05-22T14:35:30.823067Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, batch_size):\n    model.train()\n    running_loss = 0.0\n    dice = 0.0\n    iou = 0.0\n    for X_batch, Y_batch in train_loader:\n        inputs = X_batch.to(DEVICE)\n        labels = Y_batch.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs,labels.long())\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.detach().cpu().numpy() \n        dice += dice_metric(outputs, labels)\n        iou += IoU(outputs, labels)\n    train_loss = running_loss / len(train_loader)\n    train_dice = dice / len(train_loader)\n    train_IoU = iou / len(train_loader)\n    return train_loss, train_dice, train_IoU","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:52:43.740592Z","iopub.execute_input":"2022-05-18T19:52:43.741161Z","iopub.status.idle":"2022-05-18T19:52:43.748768Z","shell.execute_reply.started":"2022-05-18T19:52:43.741121Z","shell.execute_reply":"2022-05-18T19:52:43.747729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_epoch(model, val_loader, criterion, batch_size):\n    model.eval()\n    dice = 0.0\n    iou = 0.0\n    running_loss = 0.0\n    for X_batch, Y_batch in val_loader:\n        inputs = X_batch.to(DEVICE)\n        labels = Y_batch.to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs,labels.long())\n            running_loss += loss.item()\n            dice += dice_metric(outputs, labels)\n            iou += IoU(outputs, labels)\n    val_loss = running_loss / len(val_loader)\n    val_dice = dice / len(val_loader)\n    val_IoU = iou / len(val_loader)\n    return val_loss, val_dice, val_IoU","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:52:39.614759Z","iopub.execute_input":"2022-05-18T19:52:39.615568Z","iopub.status.idle":"2022-05-18T19:52:39.62513Z","shell.execute_reply.started":"2022-05-18T19:52:39.615528Z","shell.execute_reply":"2022-05-18T19:52:39.623476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size,num_workers = 2, shuffle=True)#,collate_fn = collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers = 2, shuffle=False)#,collate_fn = collate_fn)\n    history = []\n    maxIoU = 0\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} train_dice: {t_dice:0.4f} train_IoU: {t_iou:0.4f}\\\n    \\nValidation  val_loss: {v_loss:0.4f} val_dice: {v_dice:0.4f} val_IoU: {v_iou: 0.4f}\"\n    criterion = nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss()\n    \n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        params = model.parameters()\n        opt = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(opt,\n                                                   step_size=20,\n                                                   gamma=0.1)\n        for epoch in range(epochs):\n            train_loss, train_dice, train_IoU = fit_epoch(model, train_loader, criterion, opt, batch_size)\n            lr_scheduler.step()    \n            val_loss, val_dice, val_IoU = eval_epoch(model, val_loader, criterion, batch_size)\n            history.append((train_loss, train_dice, train_IoU, val_loss, val_dice, val_IoU))\n            if val_IoU > maxIoU:\n                maxIoU = val_IoU\n                torch.save(model, './weight_unet.dat')\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, t_dice = train_dice, t_iou = train_IoU, \\\n                                           v_loss=val_loss, v_dice = val_acc, v_iou = val_IoU ))   \n    return history","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-18T19:40:32.38369Z","iopub.execute_input":"2022-05-18T19:40:32.384044Z","iopub.status.idle":"2022-05-18T19:40:32.399085Z","shell.execute_reply.started":"2022-05-18T19:40:32.384007Z","shell.execute_reply":"2022-05-18T19:40:32.398032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_ious(pred, label, classes=[1], ignore_index=255, only_present=True):\n    '''computes iou for one ground truth mask and predicted mask'''\n    pred[label == ignore_index] = 0\n    ious = []\n    for c in classes:\n        label_c = label == c\n        if only_present and np.sum(label_c) == 0:\n            ious.append(np.nan)\n            continue\n        pred_c = pred == c\n        intersection = np.logical_and(pred_c, label_c).sum()\n        union = np.logical_or(pred_c, label_c).sum()\n        if union != 0:\n            ious.append(intersection / union)\n    return ious if ious else [1]\n\ndef IoU(outputs, labels, classes=None):\n    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n    ious = []\n    preds = np.copy(outputs) # copy is imp\n    labels = np.array(labels) # tensor to np\n    for pred, label in zip(preds, labels):\n        ious.append(np.nanmean(compute_ious(pred, label, classes=[1])))\n    iou = np.nanmean(ious)\n    return iou\n\ndef dice_metric(probability, truth, threshold=0.5, reduction='none'):\n    batch_size = len(truth)\n    with torch.no_grad():\n        probability = probability.view(batch_size, -1)\n        truth = truth.view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = (probability > threshold).float()\n        t = truth.float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n\n        pos_index = torch.nonzero(t_sum >= 1)\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_pos = dice_pos[pos_index]\n        dice = dice_pos\n    return dice","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:39:37.196131Z","iopub.execute_input":"2022-05-18T19:39:37.19663Z","iopub.status.idle":"2022-05-18T19:39:37.209968Z","shell.execute_reply.started":"2022-05-18T19:39:37.196586Z","shell.execute_reply":"2022-05-18T19:39:37.208226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T20:08:13.170948Z","iopub.execute_input":"2022-05-18T20:08:13.171527Z","iopub.status.idle":"2022-05-18T20:08:13.525797Z","shell.execute_reply.started":"2022-05-18T20:08:13.17149Z","shell.execute_reply":"2022-05-18T20:08:13.524483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(train_dataset, val_dataset, model, epochs = 40, batch_size = 5)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T20:08:28.796432Z","iopub.execute_input":"2022-05-18T20:08:28.797644Z","iopub.status.idle":"2022-05-18T20:08:29.800086Z","shell.execute_reply.started":"2022-05-18T20:08:28.797594Z","shell.execute_reply":"2022-05-18T20:08:29.798679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_dice, train_IoU, val_loss, val_dice, val_IoU = zip(*history)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.945042Z","iopub.status.idle":"2022-05-18T00:08:33.945444Z","shell.execute_reply.started":"2022-05-18T00:08:33.945216Z","shell.execute_reply":"2022-05-18T00:08:33.945251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(30,13))\n    ax1.plot(train_loss, label='train', marker='o')\n    ax1.plot(val_loss, label='val', marker='o')\n    ax1.set_title('Loss per epoch')\n    ax1.set_ylabel('loss');\n    ax1.set_xlabel('epoch')\n    ax1.legend(), ax1.grid()\n    \n    ax2.plot(train_IoU, label='train_IoU', marker='*')\n    ax2.plot(val_IoU, label='val_IoU',  marker='*')\n    ax2.set_title('Score per epoch')\n    ax2.set_ylabel('mean IoU')\n    ax2.set_xlabel('epoch')\n    ax2.legend(), ax2.grid()\n\n    ax3.plot(train_dice, label='train_dice', marker='*')\n    ax3.plot(val_dice, label='val_dice',  marker='*')\n    ax3.set_title('Dice per epoch')\n    ax3.set_ylabel('dice')\n    ax3.set_xlabel('epoch')\n    ax3.legend(), ax3.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:08:33.950456Z","iopub.status.idle":"2022-05-18T00:08:33.950846Z","shell.execute_reply.started":"2022-05-18T00:08:33.950634Z","shell.execute_reply":"2022-05-18T00:08:33.950656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image_mask_score(model, image, mask, metric = dice_metric):\n    model.eval()\n    \n    image = image.to(DEVICE)\n    mask = mask.to(DEVICE)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        score = metric(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0)\n    return masked, score","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:04:53.049299Z","iopub.execute_input":"2022-05-22T15:04:53.049848Z","iopub.status.idle":"2022-05-22T15:04:53.055660Z","shell.execute_reply.started":"2022-05-22T15:04:53.049807Z","shell.execute_reply":"2022-05-22T15:04:53.054928Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def score(model, dataset, metric = dice_metric):\n    res_score = []\n    for i in tqdm(range(len(val_dataset))):\n        img, mask = dataset[i]\n        pred_mask, score = predict_image_mask_score(model, img, mask, metric)\n        res_score.append(score)\n    return res_score","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:07:07.750157Z","iopub.execute_input":"2022-05-22T15:07:07.750498Z","iopub.status.idle":"2022-05-22T15:07:07.758656Z","shell.execute_reply.started":"2022-05-22T15:07:07.750457Z","shell.execute_reply":"2022-05-22T15:07:07.757568Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"model = torch.load('../input/unet-weight/weight_unet_wew.dat')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:36:14.672272Z","iopub.execute_input":"2022-05-22T14:36:14.672534Z","iopub.status.idle":"2022-05-22T14:36:18.892261Z","shell.execute_reply.started":"2022-05-22T14:36:14.672506Z","shell.execute_reply":"2022-05-22T14:36:18.891515Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(np.mean(score(model, val_dataset)))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:07:11.322781Z","iopub.execute_input":"2022-05-22T15:07:11.323139Z","iopub.status.idle":"2022-05-22T15:07:56.740215Z","shell.execute_reply.started":"2022-05-22T15:07:11.323091Z","shell.execute_reply":"2022-05-22T15:07:56.739079Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"k1,k2 = int(np.random.uniform(0,100)),int(np.random.uniform(0,100))\nimage1, mask1 = val_dataset[k1]\npred_mask1, score1 = predict_image_mask_score(model, image1, mask1)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(25,13))\nax1.imshow(np.rollaxis(image1.numpy(), 0, 3))\nax1.set_title('Picture');\nax1.set_axis_off()\n\nax2.imshow(mask1)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask1)\nax3.set_title('UNet | '+ str(score1))\nax3.set_axis_off()\n\nimage2, mask2 = val_dataset[k2]\npred_mask2, score2 = predict_image_mask_score(model, image2, mask2)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(25,13))\nax1.imshow(np.rollaxis(image2.numpy(), 0, 3))\nax1.set_title('Picture');\nax1.set_axis_off()\n\nax2.imshow(mask2)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask2)\nax3.set_title('UNet | '+str(score2))\nax3.set_axis_off()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:04:57.816333Z","iopub.execute_input":"2022-05-22T15:04:57.816617Z","iopub.status.idle":"2022-05-22T15:06:06.665585Z","shell.execute_reply.started":"2022-05-22T15:04:57.816570Z","shell.execute_reply":"2022-05-22T15:06:06.664863Z"},"trusted":true},"execution_count":71,"outputs":[]}]}
