{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport os\nimport torch\nimport random\nimport pickle\nimport functools\nimport numpy as np \nimport pandas as pd \nfrom PIL import Image\nimport torch.nn as nn \nfrom pathlib import Path\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt \nfrom tqdm import tqdm, tqdm_notebook\nfrom torchvision import transforms as T\nimport torch.utils.model_zoo as model_zoo\nfrom torchvision.models.resnet import ResNet\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.resnet import BasicBlock\nfrom torchvision.models.resnet import Bottleneck\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"527c5b86-199c-47c0-817d-fdaf62e989eb","_cell_guid":"aee6e758-eacb-4feb-99d0-ceb3a28d42b2","execution":{"iopub.status.busy":"2022-06-03T01:28:08.812252Z","iopub.execute_input":"2022-06-03T01:28:08.812574Z","iopub.status.idle":"2022-06-03T01:28:29.353463Z","shell.execute_reply.started":"2022-06-03T01:28:08.812484Z","shell.execute_reply":"2022-06-03T01:28:29.352632Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\nd = {'ImageId': df['ImageId'].unique()}\ntrain_df = pd.DataFrame(d)\ntrain_df['ClassesList'] = ['' for i in range(len(train_df))]\ntrain_df['EncodedPixels'] = ['' for i in range(len(train_df))]\nfor i in range(len(train_df)):\n    train_df['ClassesList'][i] = df.loc[df['ImageId']==train_df['ImageId'][i]]['ClassId'].values.tolist()\n    train_df['EncodedPixels'][i] = df.loc[df['ImageId']==train_df['ImageId'][i]]['EncodedPixels'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:29.355346Z","iopub.execute_input":"2022-06-03T01:28:29.355778Z","iopub.status.idle":"2022-06-03T01:28:52.90863Z","shell.execute_reply.started":"2022-06-03T01:28:29.355727Z","shell.execute_reply":"2022-06-03T01:28:52.907898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_mask( encoded, shape=(1600,256)):\n    # Делим на два списка в соответствии с кодировкой\n    if isinstance(encoded, str):\n        encoded = list(map(int, encoded.split(' ')))\n#     print(encoded)\n    full,pixel,number = [],[],[]\n    [pixel.append(encoded[i]) if i%2==0 else number.append(encoded[i]) for i in range(0, len(encoded))]\n    # \"Раскрываем\" кодировку, получаем индексы закрашенных пикселей\n    k=0\n    for i in range(len(number)):\n        for j in range(number[i]):\n            ind = pixel[i]+j\n            full.append(ind-1)\n        k +=number[i]\n    # Создаем массив под готовое изображение    \n    mask = np.zeros((1600*256,1), dtype=int)\n    # Закрашиваем соответствующие пиксели\n    mask[full] = 255\n    #преобразем к размерам фотографий металла\n    res = np.reshape(mask,(1600, 256)).T\n    res = Image.fromarray(res.astype(np.uint8))\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:52.909939Z","iopub.execute_input":"2022-06-03T01:28:52.910199Z","iopub.status.idle":"2022-06-03T01:28:52.91864Z","shell.execute_reply.started":"2022-06-03T01:28:52.910166Z","shell.execute_reply":"2022-06-03T01:28:52.917828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:52.920974Z","iopub.execute_input":"2022-06-03T01:28:52.921449Z","iopub.status.idle":"2022-06-03T01:28:52.999534Z","shell.execute_reply.started":"2022-06-03T01:28:52.921413Z","shell.execute_reply":"2022-06-03T01:28:52.998758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# разные режимы датасета \nDATA_MODES = ['train', 'val', 'test']\n# все изображения будут масштабированы к размеру 224x224 px\nRESCALE_SIZE_1 = 800\nRESCALE_SIZE_2 = 128\n# работаем на видеокарте\nDEVICE = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.001495Z","iopub.execute_input":"2022-06-03T01:28:53.002211Z","iopub.status.idle":"2022-06-03T01:28:53.00761Z","shell.execute_reply.started":"2022-06-03T01:28:53.002167Z","shell.execute_reply":"2022-06-03T01:28:53.006933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SteelDataset(Dataset):\n    def __init__(self, names, df, mode):\n        super().__init__()\n        self.names = names\n        self.mode = mode\n        if self.mode != 'test':\n            self.df = df\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.names)\n        \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file, mode):\n        if mode == 'test':\n            image = Image.open('../input/severstal-steel-defect-detection/test_images/'+ file).convert(\"RGB\")\n        else:\n            image = Image.open('../input/severstal-steel-defect-detection/train_images/'+ file).convert(\"RGB\")\n        image.load()\n        return image\n    \n    def __getitem__(self, index):\n            transforms_tens = T.Compose([\n            T.ToTensor()])\n\n            # загружаем и меняем размер изображения\n            img = self._prepare_sample(self.load_sample(self.names[index], self.mode))\n\n            if self.mode == 'test':\n                img = np.array(img)\n                max_value = 256 ** ((img.dtype == np.uint16) + 1) - 1\n                img = (img / max_value).astype(np.float32)\n                img = transforms_tens(img)\n                return img\n            \n            # загружаем классы и маски\n            labels = list(self.df['ClassesList'].loc[self.df['ImageId'] == self.names[index]])[0]\n            mask = list(self.df['EncodedPixels'].loc[self.df['ImageId'] == self.names[index]])[0]\n            num_obj = len(labels)\n            masks = np.zeros((RESCALE_SIZE_2, RESCALE_SIZE_1, 4), dtype=np.float32)\n            masks = np.transpose(masks,(2, 0, 1))\n            for i in range(4):\n                for j in range(num_obj):\n                    if i == (labels[j]-1):\n                # раскодирование масок\n                        masks[i] = np.array(self._prepare_sample(make_mask(mask[j])))\n                        masks[i] = masks[i] / 255\n\n            # преобазование в тензоры\n            masks = torch.as_tensor(masks, dtype=torch.float32)\n            label = torch.as_tensor(labels)\n            \n            # преобразуем изображения и маски\n            if self.mode == \"train\":\n                img, masks= transforms_all(img,masks)\n                \n            img = np.array(img)\n            max_value = 256 ** ((img.dtype == np.uint16) + 1) - 1\n            img = (img / max_value).astype(np.float32)\n            img = transforms_tens(img)\n            \n            return img, masks\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE_1, RESCALE_SIZE_2))\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:40:47.8205Z","iopub.execute_input":"2022-06-03T01:40:47.821189Z","iopub.status.idle":"2022-06-03T01:40:47.84032Z","shell.execute_reply.started":"2022-06-03T01:40:47.821141Z","shell.execute_reply":"2022-06-03T01:40:47.839628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transforms_all(image, segmentation):\n    if random.random() > 0.5:\n        image = TF.autocontrast(image)\n    if random.random() > 0.5:\n        image = TF.hflip(image)\n        segmentation = TF.hflip(segmentation)\n    if random.random() > 0.5:\n        image = TF.vflip(image)\n        segmentation = TF.vflip(segmentation)\n    return image, segmentation","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.031144Z","iopub.execute_input":"2022-06-03T01:28:53.031446Z","iopub.status.idle":"2022-06-03T01:28:53.042301Z","shell.execute_reply.started":"2022-06-03T01:28:53.03141Z","shell.execute_reply":"2022-06-03T01:28:53.041517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_val_names = train_df['ImageId']\ntrain_files,val_files = train_test_split(train_val_names, train_size=0.75)\nval_dataset = SteelDataset(list(val_files), train_df,  mode='val')\ntrain_dataset = SteelDataset(list(train_files),train_df, mode='train')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:40:52.020744Z","iopub.execute_input":"2022-06-03T01:40:52.021289Z","iopub.status.idle":"2022-06-03T01:40:52.030355Z","shell.execute_reply.started":"2022-06-03T01:40:52.021251Z","shell.execute_reply":"2022-06-03T01:40:52.029487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, batch_size):\n    model.train()\n    running_loss = 0.0\n    dice = 0.0\n    iou = 0.0\n    for X_batch, Y_batch in train_loader:\n        inputs = X_batch.to(DEVICE)\n        labels = Y_batch.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs,labels.long())\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.detach().cpu().numpy() \n        dice += dice_metric(outputs, labels)\n    train_loss = running_loss / len(train_loader)\n    train_dice = dice / len(train_loader)\n    return train_loss, train_dice","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:29:20.450085Z","iopub.execute_input":"2022-06-03T01:29:20.450338Z","iopub.status.idle":"2022-06-03T01:29:20.457821Z","shell.execute_reply.started":"2022-06-03T01:29:20.450308Z","shell.execute_reply":"2022-06-03T01:29:20.45683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_epoch(model, val_loader, criterion, batch_size):\n    model.eval()\n    dice = 0.0\n    iou = 0.0\n    running_loss = 0.0\n    for X_batch, Y_batch in val_loader:\n        inputs = X_batch.to(DEVICE)\n        labels = Y_batch.to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs,labels.long())\n            running_loss += loss.item()\n            dice += dice_metric(outputs, labels)\n    val_loss = running_loss / len(val_loader)\n    val_dice = dice / len(val_loader)\n\n    return val_loss, val_dice","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:29:23.758117Z","iopub.execute_input":"2022-06-03T01:29:23.758375Z","iopub.status.idle":"2022-06-03T01:29:23.7654Z","shell.execute_reply.started":"2022-06-03T01:29:23.758347Z","shell.execute_reply":"2022-06-03T01:29:23.764715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size,num_workers = 2, shuffle=True)#,collate_fn = collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers = 2, shuffle=False)#,collate_fn = collate_fn)\n    history = []\n    maxIoU = 0\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} train_dice: {t_dice:0.4f} \\\n    \\nValidation  val_loss: {v_loss:0.4f} val_dice: {v_dice:0.4f} \"\n    criterion = nn.BCEWithLogitsLoss()\n    \n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        params = model.parameters()\n        opt = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(opt,\n                                                   step_size=20,\n                                                   gamma=0.1)\n        for epoch in range(epochs):\n            train_loss, train_dice = fit_epoch(model, train_loader, criterion, opt, batch_size)\n            lr_scheduler.step()    \n            val_loss, val_dice = eval_epoch(model, val_loader, criterion, batch_size)\n            history.append((train_loss, train_dice, val_loss, val_dice))\n            if val_IoU > maxIoU:\n                maxIoU = val_IoU\n                torch.save(model, './weight_unet.dat')\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, t_dice = train_dice, \\\n                                           v_loss=val_loss, v_dice = val_acc ))   \n    return history","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-03T01:29:25.703388Z","iopub.execute_input":"2022-06-03T01:29:25.704061Z","iopub.status.idle":"2022-06-03T01:29:25.71363Z","shell.execute_reply.started":"2022-06-03T01:29:25.704023Z","shell.execute_reply":"2022-06-03T01:29:25.71295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_metric(probability, truth, threshold=0.5, reduction='none'):\n    batch_size = len(truth)\n    with torch.no_grad():\n        probability = probability.view(batch_size, -1)\n        truth = truth.view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = (probability > threshold).float()\n        t = truth.float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        neg_index = torch.nonzero(t_sum == 0)\n        pos_index = torch.nonzero(t_sum >= 1)\n\n        dice_neg = (p_sum == 0).float()\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_neg = dice_neg[neg_index]\n        dice_pos = dice_pos[pos_index]\n        dice = dice_pos\n    return dice","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.681282Z","iopub.status.idle":"2022-06-03T01:28:53.681681Z","shell.execute_reply.started":"2022-06-03T01:28:53.681462Z","shell.execute_reply":"2022-06-03T01:28:53.681484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(x, mean=None, std=None, input_space='RGB', input_range=None, **kwargs):\n\n    if input_space == 'BGR':\n        x = x[..., ::-1].copy()\n\n    if input_range is not None:\n        if x.max() > 1 and input_range[1] == 1:\n            x = x / 255.\n\n    if mean is not None:\n        mean = np.array(mean)\n        x = x - mean\n\n    if std is not None:\n        std = np.array(std)\n        x = x / std\n\n    return x\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\nclass Conv2dReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n                 stride=1, use_batchnorm=True, **batchnorm_params):\n\n        super().__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride=stride, padding=padding, bias=not (use_batchnorm)),\n            nn.ReLU(inplace=True),\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass EncoderDecoder(Model):\n\n    def __init__(self, encoder, decoder, activation):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == 'softmax':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError('Activation should be \"sigmoid\"/\"softmax\"/callable/None')\n\n    def forward(self, x):\n        \"\"\"Sequentially pass `x` trough model`s `encoder` and `decoder` (return logits!)\"\"\"\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n    def predict(self, x):\n        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)`\n        and apply activation function (if activation is not `None`) with `torch.no_grad()`\n\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n            if self.activation:\n                x = self.activation(x)\n\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        super().__init__()\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.block(x)\n        return x\n\n\nclass CenterBlock(DecoderBlock):\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UnetDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels=(256, 128, 64, 32, 16),\n            final_channels=1,\n            use_batchnorm=True,\n            center=False,\n    ):\n        super().__init__()\n\n        if center:\n            channels = encoder_channels[0]\n            self.center = CenterBlock(channels, channels, use_batchnorm=use_batchnorm)\n        else:\n            self.center = None\n\n        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n        out_channels = decoder_channels\n\n        self.layer1 = DecoderBlock(in_channels[0], out_channels[0], use_batchnorm=use_batchnorm)\n        self.layer2 = DecoderBlock(in_channels[1], out_channels[1], use_batchnorm=use_batchnorm)\n        self.layer3 = DecoderBlock(in_channels[2], out_channels[2], use_batchnorm=use_batchnorm)\n        self.layer4 = DecoderBlock(in_channels[3], out_channels[3], use_batchnorm=use_batchnorm)\n        self.layer5 = DecoderBlock(in_channels[4], out_channels[4], use_batchnorm=use_batchnorm)\n        self.final_conv = nn.Conv2d(out_channels[4], final_channels, kernel_size=(1, 1))\n\n        self.initialize()\n\n    def compute_channels(self, encoder_channels, decoder_channels):\n        channels = [\n            encoder_channels[0] + encoder_channels[1],\n            encoder_channels[2] + decoder_channels[0],\n            encoder_channels[3] + decoder_channels[1],\n            encoder_channels[4] + decoder_channels[2],\n            0 + decoder_channels[3],\n        ]\n        return channels\n\n    def forward(self, x):\n        encoder_head = x[0]\n        skips = x[1:]\n\n        if self.center:\n            encoder_head = self.center(encoder_head)\n\n        x = self.layer1([encoder_head, skips[0]])\n        x = self.layer2([x, skips[1]])\n        x = self.layer3([x, skips[2]])\n        x = self.layer4([x, skips[3]])\n        x = self.layer5([x, None])\n        x = self.final_conv(x)\n\n        return x\n\n\nclass ResNetEncoder(ResNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.fc\n\n    def forward(self, x):\n        x0 = self.conv1(x)\n        x0 = self.bn1(x0)\n        x0 = self.relu(x0)\n\n        x1 = self.maxpool(x0)\n        x1 = self.layer1(x1)\n\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return [x4, x3, x2, x1, x0]\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('fc.bias')\n        state_dict.pop('fc.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nresnet_encoders = {\n    'resnet18': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet18'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [2, 2, 2, 2],\n        },\n    },\n    \n    'resnet50': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n}\n\nencoders = {}\nencoders.update(resnet_encoders)\n\ndef get_encoder(name, encoder_weights=None):\n    Encoder = encoders[name]['encoder']\n    encoder = Encoder(**encoders[name]['params'])\n    encoder.out_shapes = encoders[name]['out_shapes']\n\n    if encoder_weights is not None:\n        settings = encoders[name]['pretrained_settings'][encoder_weights]\n        encoder.load_state_dict(model_zoo.load_url(settings['url']))\n\n    return encoder\n\n\ndef get_encoder_names():\n    return list(encoders.keys())\n\n\ndef get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n    settings = encoders[encoder_name]['pretrained_settings']\n\n    if pretrained not in settings.keys():\n        raise ValueError('Avaliable pretrained options {}'.format(settings.keys()))\n\n    input_space = settings[pretrained].get('input_space')\n    input_range = settings[pretrained].get('input_range')\n    mean = settings[pretrained].get('mean')\n    std = settings[pretrained].get('std')\n    \n    return functools.partial(preprocess_input, mean=mean, std=std, input_space=input_space, input_range=input_range)\n\n\nclass Unet(EncoderDecoder):\n\n    def __init__(\n            self,\n            encoder_name='resnet34',\n            encoder_weights='imagenet',\n            decoder_use_batchnorm=True,\n            decoder_channels=(256, 128, 64, 32, 16),\n            classes=1,\n            activation='sigmoid',\n            center=False,  # usefull for VGG models\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = UnetDecoder(\n            encoder_channels=encoder.out_shapes,\n            decoder_channels=decoder_channels,\n            final_channels=classes,\n            use_batchnorm=decoder_use_batchnorm,\n            center=center,\n        )\n\n        super().__init__(encoder, decoder, activation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)\n# model = Unet(\"resnet50\", encoder_weights=\"imagenet\", classes=4, activation=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(train_dataset, val_dataset, model, epochs = 40, batch_size = 5)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:29:40.72173Z","iopub.execute_input":"2022-06-03T01:29:40.722287Z","iopub.status.idle":"2022-06-03T01:34:48.318026Z","shell.execute_reply.started":"2022-06-03T01:29:40.722246Z","shell.execute_reply":"2022-06-03T01:34:48.316714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_acc, val_loss, val_acc = zip(*history)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.700235Z","iopub.status.idle":"2022-06-03T01:28:53.700943Z","shell.execute_reply.started":"2022-06-03T01:28:53.700694Z","shell.execute_reply":"2022-06-03T01:28:53.70072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,13))\n    ax1.plot(train_loss, label='train', marker='o')\n    ax1.plot(val_loss, label='val', marker='o')\n    ax1.set_title('Loss per epoch')\n    ax1.set_ylabel('loss');\n    ax1.set_xlabel('epoch')\n    ax1.legend(), ax1.grid()\n    \n    ax2.plot(train_dice, label='train_dice', marker='*')\n    ax2.plot(val_dice, label='val_dice',  marker='*')\n    ax2.set_title('Score per epoch')\n    ax2.set_ylabel('mean dice')\n    ax2.set_xlabel('epoch')\n    ax2.legend(), ax2.grid()\n    \n#     plt.savefig('0.710615.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.706276Z","iopub.status.idle":"2022-06-03T01:28:53.706675Z","shell.execute_reply.started":"2022-06-03T01:28:53.706457Z","shell.execute_reply":"2022-06-03T01:28:53.706479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image_mask_miou(model, image, mask):\n    model.eval()\n    \n    image = image.to(DEVICE)\n    mask = mask.to(DEVICE)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        print(output)\n        score = dice_metric(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0)\n    return masked, score","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.707846Z","iopub.status.idle":"2022-06-03T01:28:53.70882Z","shell.execute_reply.started":"2022-06-03T01:28:53.708562Z","shell.execute_reply":"2022-06-03T01:28:53.708587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def miou_score(model, val_dataset):\n    score_iou = []\n    for i in tqdm(range(len(val_dataset))):\n        img, mask = val_dataset[i]\n        pred_mask, score = predict_image_mask_miou(model, img, mask)\n        score_iou.append(score)\n    return score_iou","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.710223Z","iopub.status.idle":"2022-06-03T01:28:53.710621Z","shell.execute_reply.started":"2022-06-03T01:28:53.710403Z","shell.execute_reply":"2022-06-03T01:28:53.710425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = torch.load('../input/unet-weight/weight_unet_wew.dat')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.714024Z","iopub.status.idle":"2022-06-03T01:28:53.714435Z","shell.execute_reply.started":"2022-06-03T01:28:53.714219Z","shell.execute_reply":"2022-06-03T01:28:53.714241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.mean(miou_score(model, val_dataset)))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T01:28:53.715966Z","iopub.status.idle":"2022-06-03T01:28:53.716372Z","shell.execute_reply.started":"2022-06-03T01:28:53.716157Z","shell.execute_reply":"2022-06-03T01:28:53.71618Z"},"trusted":true},"execution_count":null,"outputs":[]}]}